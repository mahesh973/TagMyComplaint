{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd15714-e39c-410a-b2ba-1ef48df1c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Extracts a subset of columns from a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame from which columns need to be extracted.\n",
    "        columns (list): A list of column names to extract.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame containing only the specified columns.\n",
    "    \"\"\"\n",
    "    return df[columns].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531f60e3-cf92-47d0-bb12-df5aa0d7f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_add_values(original_df, extra_df, main_column, filter_values):\n",
    "    \"\"\"\n",
    "    Filters values from an extra DataFrame based on a main column and specific values,\n",
    "    then adds the filtered values to the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        original_df (DataFrame): The original DataFrame to which filtered values will be added.\n",
    "        extra_df (DataFrame): The extra DataFrame from which values will be filtered.\n",
    "        main_column (str): The main column in both DataFrames used for filtering.\n",
    "        filter_values (list): The values to filter from the extra DataFrame and add to the original DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The updated original DataFrame with filtered values added.\n",
    "    \"\"\"\n",
    "    filtered_values = extra_df[extra_df[main_column].isin(filter_values)].copy()\n",
    "    return pd.concat([original_df, filtered_values], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6841c8-5fc2-45fb-9e81-cd2c1696b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_balanced_data(df, product_column, subproduct_column, sample_size):\n",
    "    \"\"\"\n",
    "    Samples data points from a DataFrame while balancing subproducts within each product category.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame from which data points will be sampled.\n",
    "        product_column (str): The column name representing the product category.\n",
    "        subproduct_column (str): The column name representing the subproduct category.\n",
    "        sample_size (int): The number of data points to sample for each product.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A sampled DataFrame containing the specified number of data points with balanced subproducts.\n",
    "    \"\"\"\n",
    "    sampled_dfs = []\n",
    "    for product in df[product_column].unique():\n",
    "        product_df = df[df[product_column] == product]\n",
    "        if len(product_df) <= sample_size:\n",
    "            sampled_dfs.append(product_df)\n",
    "        else:\n",
    "            subproduct_counts = product_df[subproduct_column].value_counts()\n",
    "            sample_per_subproduct = {subproduct: min(sample_size, count) for subproduct, count in subproduct_counts.items()}\n",
    "            sampled_product_df = pd.concat([product_df[product_df[subproduct_column] == subproduct].sample(n=count, replace=True, random_state=42) for subproduct, count in sample_per_subproduct.items()])\n",
    "            sampled_dfs.append(sampled_product_df)\n",
    "    sampled_df = pd.concat(sampled_dfs)\n",
    "    return sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280a5e9c-8adb-463a-85f7-2065877a4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivanimundle/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dataframes_and_split(data, folder_path, test_size=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Creates dataframes for each product category, splits them into train and validation sets, and saves them to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): The original DataFrame containing all data.\n",
    "        folder_path (str): The path to the folder where CSV files will be saved.\n",
    "        test_size (float or int): The proportion of the dataset to include in the validation split, or the absolute number of samples.\n",
    "        random_state (int or None): Controls the randomness of the training and validation data splitting.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing train and validation DataFrames for each product category.\n",
    "    \"\"\"\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # Group data by product category\n",
    "    grouped_data = data.groupby('Product')\n",
    "    \n",
    "    train_validation_splits = {}\n",
    "    for product, group_df in grouped_data:\n",
    "        # Save DataFrame for this product category\n",
    "        product_file_path = os.path.join(folder_path, f\"{product.replace('/', '_').replace(' ', '_').lower()}_data.csv\")\n",
    "        group_df.to_csv(product_file_path, index=False)\n",
    "\n",
    "        # Split the DataFrame into train and validation sets\n",
    "        train_df, val_df = train_test_split(group_df, test_size=test_size, random_state=random_state, stratify=group_df['Sub-product'])\n",
    "        \n",
    "        # Save train DataFrame\n",
    "        train_file_path = os.path.join(folder_path, f\"{product.replace('/', '_').replace(' ', '_').lower()}_train_data.csv\")\n",
    "        train_df.to_csv(train_file_path, index=False)\n",
    "\n",
    "        # Save validation DataFrame\n",
    "        val_file_path = os.path.join(folder_path, f\"{product.replace('/', '_').replace(' ', '_').lower()}_val_data.csv\")\n",
    "        val_df.to_csv(val_file_path, index=False)\n",
    "\n",
    "        # Store train and validation DataFrames in the dictionary\n",
    "        train_validation_splits[product] = {'train': train_df, 'validation': val_df}\n",
    "    \n",
    "    return train_validation_splits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936902a2-ff1c-422b-b8d5-1ac7897eac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_dataframe_to_csv(df, file_name, directory='./'):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame to be saved.\n",
    "        file_name (str): The name of the CSV file.\n",
    "        directory (str): The directory where the CSV file will be saved. Defaults to the current directory.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Concatenate the directory and file name\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb94a4e1-eb4f-4d29-bbec-9ab79d4385f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_balanced_data(df_2023, df_2022, product_column, products_to_include, sample_size, directory_to_save='./data_splits/'):\n",
    "    # Extract selected columns\n",
    "    selected_columns = ['Consumer complaint narrative', product_column, 'Sub-product']\n",
    "    product_training_2023 = extract_columns(df_2023, selected_columns)\n",
    "    product_training_2022 = extract_columns(df_2022, selected_columns)\n",
    "    \n",
    "    # Filter and add values from 2022 to 2023 data\n",
    "    balanced_df = filter_and_add_values(product_training_2023, product_training_2022, product_column, products_to_include)\n",
    "    \n",
    "    # Sample balanced data\n",
    "    sampled_df = sample_balanced_data(balanced_df, product_column, 'Sub-product', sample_size)\n",
    "    \n",
    "    # Save sampled data to CSV\n",
    "    file_name = 'train-data-balanced.csv'\n",
    "    save_dataframe_to_csv(sampled_df, file_name, directory_to_save)\n",
    "    return sampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c158b942-13f7-4293-a9fe-61d393b039d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_to_save = './data_splits/'\n",
    "training_2023 = pd.read_csv('data_splits/train-data-split_2023.csv')\n",
    "training_2022 = pd.read_csv('data_splits/train-data-split_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad76278-a3b3-4ed3-9e2c-2eda6cc5fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_to_include = ['Loans / Mortgage', 'Credit/Prepaid Card', 'Checking or savings account']\n",
    "sampled_df=create_balanced_data(training_2023, training_2022, 'Product', products_to_include, 15000, directory_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7db6ae2-2b07-420f-a2eb-4663bdbec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'product_data_splits'\n",
    "train_validation_splits = create_dataframes_and_split(sampled_df, folder_path, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b915ef2-d27f-4c0a-8266-d17ce4bed88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
